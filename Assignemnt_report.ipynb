{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b475157a-2c50-4a5f-9dd0-31fc7d3ce423",
   "metadata": {},
   "source": [
    "## <Center> Detailed Report of the Excercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b916ee-3cc0-43d2-a145-cc37ada082a9",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "\n",
    "* ### KMeans_Clustering_UsingGEE Steps\n",
    "* ### Assisgnment Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d12a06-a894-4c45-ad58-ae89905a3c0c",
   "metadata": {},
   "source": [
    "### <Center> KMeans_Clustering_UsingGEE Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47d8637-228d-4676-b48a-e9a0f2cb59ac",
   "metadata": {},
   "source": [
    "In this excercise, our task is to perform K-means clustering algorithm for Landuse clustering in two different countries using the Sentinel 2 imagery and functionalities of Google earth Engine. Several steps has been performed to carry out the task. The steps are discussed below along with arough flowchart for better understanding the steps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59e375eb-54ae-444c-9ad7-7b8c7b52e8a9",
   "metadata": {},
   "source": [
    "<Center> \n",
    "  <img src=\"./images/Flowchart.jpg\" alt=\"Image 1\" style=\"width: 49%; margin: 0 10px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c452be0d-eecd-485d-9917-57900ba7a210",
   "metadata": {},
   "source": [
    "## 1. Import Google Earth Engine API and Initializing the authentication process\n",
    "\n",
    "This step has been carried out to import all the functionalities of Google Earth Engine in Python and the authentication step lets one to log in Google Earth Engine with one's user name and password. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890daff3-4731-4f72-9a82-5912c10f1e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07182bac-bb65-4980-9627-8ec50627099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ee.Initialize()\n",
    "except Exception as e:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5e8ed1-a29b-4064-ac3a-e6244b28b246",
   "metadata": {},
   "source": [
    "## 2. Dask\n",
    "\n",
    "As the sample dataset contains 10 CSV files, dask is used to read all the dataset at once and to minimize the processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc2dabf-4559-4063-b8a2-1890b6f60320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0d13aa-a80f-4617-b2f9-98fc54dd9ba3",
   "metadata": {},
   "source": [
    "## 3. Reading Sample Dataset and Reindexing\n",
    "\n",
    "As all the sample dataset CSV file has its own indexing, a reindexing method was required to create and compile it into one CSV file.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47d0f82-ee00-49f2-b7e5-4a8ef7a7d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv('Data/SamplesSet*.csv')\n",
    "df = df.compute()\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6a9db7-9d26-4b43-9d85-888540b15097",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis\n",
    "\n",
    "In this step, several exploratory data analyzing processes have been carried out.\n",
    " * **Dropping Duplicates** - To remove duplicate cells\n",
    " * **Distinct_Value_Count** - To find out the numbe rof distinct landcover classes. 5 landcover class have been found.\n",
    " * **Minimum and Maximum Latitude and Longitude Value** - To check the range of countries/geographies. In this excercise, we found from the minimum and maxium latitude and longitude that the dataset given was collected from Europe. \n",
    " * **Plotting the top 20 countries with most location from dataset** - This steps has been perfomed to check the where/which country these sample datasets mostly belong to. It helped us to choose our study area for this excercise. As in this study, we were asked to choose two countries, one of which has larger correspondence to the dataset and one has lower correspondence. We chose, **France** for higher similarity and **Poland** for Lower similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8290e8ce-0203-40d5-b24d-e7646f71f97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_values = df['landcover'].drop_duplicates()\n",
    "distinct_value_counts = df['landcover'].value_counts()\n",
    "min_value_lat = df['lat'].min()\n",
    "max_value_lat = df['lat'].max()\n",
    "min_value_lon = df['lon'].min()\n",
    "max_value_lon= df['lon'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178b8ed4-06d0-4c4b-9629-86a1ce906335",
   "metadata": {},
   "source": [
    "## 5. Importing Bounding Box List \n",
    "\n",
    "A bounding box list of the countries has been accessed and it leads to the task of finding the bounding box where majority of the sample reference points belong to.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc4bb3-0ca0-4a94-8306-a336f77ff361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('bounding_boxes.json', 'r') as file:\n",
    "    bounding_boxes = json.load(file)\n",
    "\n",
    "def is_in_bounding_box(lat, lon, bounding_box):\n",
    "    sw = bounding_box['sw']\n",
    "    ne = bounding_box['ne']\n",
    "    return sw['lat'] <= lat <= ne['lat'] and sw['lon'] <= lon <= ne['lon']\n",
    "\n",
    "\n",
    "dfs = []\n",
    "\n",
    "# count locations in each bounding box\n",
    "for code, box in bounding_boxes.items():\n",
    "    country_name = code\n",
    "    bounding_box = box\n",
    "    min_lat, min_lon = bounding_box['sw']['lat'], bounding_box['sw']['lon']\n",
    "    max_lat, max_lon = bounding_box['ne']['lat'], bounding_box['ne']['lon']\n",
    "    filtered_df = df[df.apply(lambda row: is_in_bounding_box(row['lat'], row['lon'], bounding_box), axis=1)]\n",
    "    count = len(filtered_df)\n",
    "    bounding_box_df = dd.from_pandas(pd.DataFrame({'Country': [country_name], 'Code': [code], 'Count': [count]}), npartitions=1)\n",
    "    dfs.append(bounding_box_df)\n",
    "\n",
    "counts_df = dd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce021bad-dab3-4644-a3d7-1679ec4859b7",
   "metadata": {},
   "source": [
    "## 6. Filtering the point within Bounding box of study area\n",
    "\n",
    "With this operation, the reference points has bben cropped within the bounding box for France and Poland and it is used for the accuracy assessment of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf8af0b-2873-4e24-903d-8b1c8c767692",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_latitude = 43.303\n",
    "max_latitude = 49.124\n",
    "min_longitude = -3.142\n",
    "max_longitude = 6.561\n",
    "\n",
    "# Filter points within the bounding box\n",
    "france_points_small = df[\n",
    "    (df['lat'] >= min_latitude) & \n",
    "    (df['lat'] <= max_latitude) & \n",
    "    (df['lon'] >= min_longitude) & \n",
    "    (df['lon'] <= max_longitude)\n",
    "]\n",
    "\n",
    "france_points_small = france_points_small.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e48c445-f54d-43be-aa98-129b85c73b34",
   "metadata": {},
   "source": [
    "## 7. Sentinel_2 Image Access\n",
    "\n",
    "The Sentinel-2 images for Summer,2023 (2023-06-01 to 2023-08-30) with cloud cover below 30% have been accessed for this excercise. Here, the cloud coverage below 30% is considered enough as it will be compensated by the median calculation.  The GEE Pyhton API has been used as it has massive archive of dataset and it can provide direct access to these dataset instead of downloading it on the local computer which increases the processing time and speed and also reduce the task of downloading the image manually from server. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a998199d-680e-4d3f-843a-82a1ccb02a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_france = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\")\\\n",
    "  .filterBounds(bounding_box)\\\n",
    "  .filterDate('2023-06-01', '2023-08-30')\\\n",
    "  .filterMetadata('CLOUDY_PIXEL_PERCENTAGE', 'less_than', 30) \\\n",
    "  .sort('CLOUDY_PIXEL_PERCENTAGE', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fd9f8d-3444-4f3b-a343-7625aa809c6e",
   "metadata": {},
   "source": [
    "## 8. Data Interrogotaion\n",
    "\n",
    "Here several image data interrogation steps has been perfomed to get the information about the images that have been accessed and to reduce the size of the images.\n",
    " * Number of Images\n",
    " * Band Names\n",
    " * Median Image Calculation\n",
    " * Band Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2fe7b4-8a07-41d0-ade8-2e5aaaf3bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfImages = map_france.aggregate_array('system:index').getInfo()\n",
    "bandNames = median_image.bandNames() \n",
    "median_image = map_france.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f78871d-ffc8-4764-9a93-019c90c8db37",
   "metadata": {},
   "source": [
    "## 9. Create Training datasets and display\n",
    "\n",
    "In this steps, a training datasets has been created for the selected study area for about 10000 number of pixels and this clustering will occur based on the spectral characteristics of the pixel in the feature space and their distance from the randomly initialized cluster center, and for later experiments, the number of pixels has been also increased to 100000 to check the result. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2756ae31-40c1-4a43-b8f9-a9530b92dceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = median_image.sample(**{\n",
    "    'region':bounding_box,\n",
    "    'scale': 30,\n",
    "    'numPixels': 10000,\n",
    "    'seed': 0,\n",
    "    'tileScale' : 2,\n",
    "    'geometries': True # the geometries are not included\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62587bb4-ef55-400a-ac97-02e9b690c8c6",
   "metadata": {},
   "source": [
    "## 10. Clustering and Displaying the cluster\n",
    "\n",
    "In this step, the clustering has been carried out for a several number of clusters based on the training data previously created randomly from the stallite imageries, and result will provide an image with predefined cluster for the whole images where each pixel belong to that predefined clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7c447c-0463-46f7-a29d-ec48fca42a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 5\n",
    "clusterer = ee.Clusterer.wekaKMeans(n_clusters).train(training)\n",
    "\n",
    "# Cluster the input using the trained clusterer.\n",
    "result = median_image.cluster(clusterer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2e410-5623-46e6-9383-f8f3b50709fb",
   "metadata": {},
   "source": [
    "## 11. Performance Assessment\n",
    "\n",
    "To check the accuracy of the cluster based on the ground truth data from the sample dataset, this step has been performed. The result shows inaccuracy of the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1508c759-ca6f-494e-bb43-432ffeed6c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_for_coordinates(lat, lon):\n",
    "    point = ee.Geometry.Point(lon, lat)\n",
    "    cluster = result.reduceRegion(ee.Reducer.first(), point, 80)  # Adjust the scale as needed\n",
    "    return cluster.get('cluster').getInfo()\n",
    "\n",
    "france_points['cluster'] = france_points.apply(lambda row: get_cluster_for_coordinates(row['lat'], row['lon']), axis=1)\n",
    "france_points.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934cf1de-fc55-4926-8879-65885559443c",
   "metadata": {},
   "source": [
    "## 12. Histogram Operation \n",
    "\n",
    "Histogram Operation has been performed to check the clusters and to check how many land cover classes belongs to each classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1860c5-5b95-4487-ba8b-fa0d01da3282",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = france_points.groupby(['cluster', 'landcover']).size().reset_index(name='count')\n",
    "\n",
    "# Pivot the data to create a table suitable for plotting\n",
    "pivot_data = grouped_data.pivot(index='cluster', columns='landcover', values='count').fillna(0)\n",
    "\n",
    "colors = {\n",
    "    'Water': 'blue',\n",
    "    'ForestNaturalAreas': 'green',\n",
    "    'ArtificialSurfaces': 'grey',\n",
    "    'Wetlands': 'purple',\n",
    "    'AgriculturalArea': '#FFD700'\n",
    "}\n",
    "\n",
    "# Plotting with custom colors\n",
    "pivot_data.plot(kind='bar', stacked=True, color=[colors[col] for col in pivot_data.columns])\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Points per Cluster for Each Landcover')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2421749d-4436-4350-a89c-11c2d9ff85fa",
   "metadata": {},
   "source": [
    "### <Center> Assignment Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf2de61-5278-428d-aa33-f4e4165ed450",
   "metadata": {},
   "source": [
    "## 1. Why do we need to train an Unsupervised Classifier in GEE? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a7acad-8ad1-4d7e-977a-3536f7b69720",
   "metadata": {},
   "source": [
    "Unlike supervised casuetring algorithm which requires labeled training data, any machine learning clustering algorithm in Google earth engine requires unlabeled training data simply beacuse it has large dataset from all over the world and it would be difficult and challenging due to its scale of the datsets to perform unsupervised clustering on it. Generally in any unsupervised task, we ask machine to find the pattern or cluster in dataset given which is quite difficult in Earth Engine. rather, it requires some smaller, manageable training data and then based on the trainng data, it performs the clustering for whole broader dataset for predefined cluster numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac76cbd-d870-4745-b082-77e93276d576",
   "metadata": {},
   "source": [
    "## 2. Impact of Number of Cluster in the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc67ebc-1dc3-44de-bd01-3103b7e27e0e",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "  <img src=\"images/Pol2.png\" alt=\"Image 1\" style=\"width: 49%; margin: 0 10px;\">\n",
    "  <img src=\"images/Pol3.png\" alt=\"Image 2\" style=\"width: 49%; margin: 0 10px;\">\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "  <img src=\"images/Pol4.png\" alt=\"Image 1\" style=\"width: 49%; margin: 0 10px;\">\n",
    "  <img src=\"images/Pol5.png\" alt=\"Image 1\" style=\"width: 49%; margin: 0 10px;\">\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "  <img src=\"images/Pol10.png\" alt=\"Image 1\" style=\"width: 49%; margin: 0 10px;\">\n",
    "  <img src=\"images/Pol12.png\" alt=\"Image 1\" style=\"width: 49%; margin: 0 10px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90af259c-dbba-45ce-8703-086e83c8308b",
   "metadata": {},
   "source": [
    "We have tried multiple number of clusters to check the impact in the result. Even with tryingmultiple number of clusters, we found each cluster is a combination of several number of land cover classes. There are several potential reason behind it. \n",
    "* **Cluster center Initialization** - As in K-means the cluster center initialization occurs randomly and it has influence in rest of the clustering phase by calculating the distance from that center, we believe, it creates this problem of clustering different land cover class into one cluster.\n",
    "* **Use of All Bands** - Generally, Red, Green, Blue, and NIR bands is required to distinguish different land cover as mostly they are sensitive to these bands and their reflectance value high;y dependent on it, due to the use of all the bands here, and each band is responsible for creating one dimension (and create a multi-dimension evetually) where these pixels are situated, it creates a problem while calculating the distance for K-means clustering. Thus, each clusters takes all the nearby pixel even if they belong to different class and make on cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b031cd6a-da69-4ec0-b31f-9b7e1c88418e",
   "metadata": {},
   "source": [
    "## 3. Impact of performing Cluster in different Region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aff6cc-bcb3-4a6e-bfca-92de3b24dc38",
   "metadata": {},
   "source": [
    "## 4. Computational Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd28930-cb6b-479e-ba90-62fd11680550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
